{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d69b02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "tensor([[[[-0.7308, -0.7308, -0.7308,  ..., -0.6965, -0.6965, -0.6965],\n",
      "          [-0.7308, -0.7137, -0.7137,  ..., -0.6965, -0.6965, -0.6965],\n",
      "          [-0.7308, -0.7137, -0.7137,  ..., -0.6965, -0.6965, -0.6965],\n",
      "          ...,\n",
      "          [ 0.0056,  0.0569,  0.1426,  ..., -0.2171, -0.2171, -0.2342],\n",
      "          [-0.0801,  0.0398,  0.1426,  ..., -0.2684, -0.2171, -0.2342],\n",
      "          [-0.1486, -0.1828, -0.0801,  ..., -0.3027, -0.2856, -0.2513]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8354,  0.8354,  0.8354,  ...,  0.8704,  0.8704,  0.8704],\n",
      "          [ 0.8354,  0.8529,  0.8529,  ...,  0.8704,  0.8704,  0.8704],\n",
      "          [ 0.8354,  0.8529,  0.8529,  ...,  0.8704,  0.8704,  0.8704],\n",
      "          ...,\n",
      "          [ 0.1877,  0.2752,  0.3627,  ...,  0.0126,  0.0126, -0.0049],\n",
      "          [ 0.1352,  0.2577,  0.3627,  ..., -0.0399,  0.0126, -0.0224],\n",
      "          [ 0.0651,  0.0301,  0.1352,  ..., -0.0924, -0.0749, -0.0399]]],\n",
      "\n",
      "\n",
      "        [[[-0.7587, -0.7587, -0.7587,  ..., -0.7238, -0.7238, -0.7238],\n",
      "          [-0.7587, -0.7413, -0.7413,  ..., -0.7238, -0.7238, -0.7238],\n",
      "          [-0.7587, -0.7413, -0.7413,  ..., -0.7238, -0.7238, -0.7238],\n",
      "          ...,\n",
      "          [ 0.0605,  0.1302,  0.2173,  ..., -0.2010, -0.2010, -0.2184],\n",
      "          [-0.0267,  0.0953,  0.1999,  ..., -0.2532, -0.2010, -0.2358],\n",
      "          [-0.1138, -0.1487, -0.0441,  ..., -0.2707, -0.2532, -0.2184]]]])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "\n",
    "image = []\n",
    "audio = []\n",
    "label = []\n",
    "\n",
    "data_root = '/home/ducca/CMKD/C2KD/cramed_unpair/CREMA-D/'\n",
    "class_dict = {'NEU':0, 'HAP':1, 'SAD':2, 'FEA':3, 'DIS':4, 'ANG':5}\n",
    "\n",
    "visual_feature_path = '/home/ducca/CMKD/C2KD/cramed_unpair/CREMA-D/'\n",
    "audio_feature_path = '/home/ducca/CMKD/C2KD/cramed_unpair/CREMA-D/AudioWAV/'\n",
    "\n",
    "train_csv = os.path.join(data_root, 'train.csv')\n",
    "test_csv = os.path.join(data_root, 'test.csv')\n",
    "\n",
    "# if mode == 'train':\n",
    "csv_file = train_csv\n",
    "# else:\n",
    "#     csv_file = test_csv\n",
    "\n",
    "with open(csv_file, encoding='UTF-8-sig') as f2:\n",
    "    csv_reader = csv.reader(f2)\n",
    "    for item in csv_reader:\n",
    "        audio_path = os.path.join(audio_feature_path, item[0] + '.wav')\n",
    "        visual_path = os.path.join(visual_feature_path, 'Image-{:02d}-FPS'.format(1), item[0])\n",
    "\n",
    "        if os.path.exists(audio_path) and os.path.exists(visual_path):\n",
    "            image.append(visual_path)\n",
    "            audio.append(audio_path)\n",
    "            label.append(class_dict[item[1]])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "indices_per_class = defaultdict(list)\n",
    "for idx, lbl in enumerate(label):\n",
    "    indices_per_class[lbl].append(idx)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "image_samples = os.listdir(image[100])\n",
    "select_index = np.random.choice(len(image_samples), size=1, replace=False)\n",
    "select_index.sort()\n",
    "images = torch.zeros((1, 3, 224, 224))\n",
    "for i in range(1):\n",
    "    img = Image.open(os.path.join(image[100], image_samples[i])).convert('RGB')\n",
    "    img = transform(img)\n",
    "    images[i] = img\n",
    "    print(images.shape)\n",
    "\n",
    "images = torch.permute(images, (1,0,2,3))\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918896fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c2kd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
